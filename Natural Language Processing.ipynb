{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# NLP: Analyzing Review Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### The objective is to be able to extract the positive or negative sentiment and gain insight from review text using Yelp review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Download and parse the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To start, let's download the data set from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review_reduced.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review_reduced.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "stars = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### First, I build a linear model predicting the star rating based on the text reviews (Ridge Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ToText(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [row['text'] for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text = ToText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = to_text.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<253272x20168 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11993261 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=20, max_df=0.4)\n",
    "\n",
    "cv.fit_transform(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_t = cv.fit_transform(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00am' ... 'zupas' 'zuzu' 'über']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_text = GridSearchCV(\n",
    "    cv,\n",
    "    {'min_df':[1,2,4,8,10,12,16,20,22,24,26], 'max_df':[0.4,0.5,0.6,0.7,0.8,0.9]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
       "             param_grid={'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0]})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "rg = Ridge()\n",
    "ridge = GridSearchCV(\n",
    "    rg,\n",
    "    {'alpha': [1,0.1,0.01,0.001,0.0001,0]},\n",
    "     n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge.fit(text_t, stars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "nlp_model = Pipeline([\n",
    "    ('to_text', to_text),\n",
    "    ('count_vectorizer', CountVectorizer(stop_words='english', min_df=20, max_df=0.4)),\n",
    "    ('ridge', Ridge(alpha=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('to_text', ToText()),\n",
       "                ('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.8, min_df=2, stop_words='english')),\n",
       "                ('ridge', Ridge(alpha=1))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.32622255, 4.19839414, 4.93299049, 3.52243758, 1.73235375,\n",
       "       5.06337603, 3.81810765, 3.98456226, 3.39986864, 4.22608942])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model.predict(data[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('to_text', ToText()),\n",
       "                ('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.4, min_df=20, stop_words='english')),\n",
       "                ('ridge', Ridge(alpha=1))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_model = nlp_model\n",
    "\n",
    "bag_of_words_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I'll now consider both single words and pairs of consecutive words that appear. I use a vectorizer that applies some sort of normalization, e.g., the TfidfVectorizer a word count vectorizer combined with TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=300, ngram_range=(2,2), stop_words='english', min_df=20, max_df=0.6)\n",
    "\n",
    "text_t = cv.fit_transform(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfT = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<253272x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 546620 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfT.fit_transform(text_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "bigram_model = Pipeline([\n",
    "    ('to_text', to_text),\n",
    "    ('count_vectorizer', CountVectorizer(max_features=6000, stop_words='english', min_df=10, max_df=0.8)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('ridge', Ridge(alpha=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('to_text', ToText()),\n",
       "                ('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.8, max_features=6000, min_df=10,\n",
       "                                 stop_words='english')),\n",
       "                ('tfidf', TfidfTransformer()), ('ridge', Ridge(alpha=1))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6157500905145326"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.score(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I now want to determine the most \"polarizing words\" in the corpus of reviews.  In other words, I'd like to identify words that strongly signal whether a review is either positive or negative.  For example, we understand that a word like \"terrible\" will most likely appear in negative rather than positive reviews. I'll use Naive Bayes model to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data);data\n",
    "data2 = data[data['stars'].isin([1,5])]\n",
    "stars = data2['stars']\n",
    "text = data2['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stars) == len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ng_tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdi_text = ng_tfidf.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multinom = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model = multinom.fit(text, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ToText(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X[X['stars'].isin([1,5])]\n",
    "        return X2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vect_text = Pipeline([\n",
    "    ('to_text', ToText()),\n",
    "    ('tfidfvectorizer', TfidfVectorizer(stop_words='english')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vect_text.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bayes_model = MultinomialNB(alpha=0.1).fit(vectorized, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.23160812,  -9.09214586, -14.28867132, ..., -13.23571879,\n",
       "        -13.23571879, -13.23571879],\n",
       "       [ -8.32069496,  -9.78023682, -14.91158699, ..., -15.433933  ,\n",
       "        -15.433933  , -15.433933  ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = bayes_model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity1 = log_prob[1]/log_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ng_tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., '髒桌面', '鹹豆漿除了一两个米粒大的肉鬆几乎只是油條和豆漿',\n",
       "       '麻辣3拼新价格'], dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs = list(zip(words, polarity1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs = sorted(words_probs, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = words_probs[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity2 = log_prob[0]/log_prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs2 = list(zip(words, polarity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_probs2 = sorted(words_probs2, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = words_probs2[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars =  pos_words + neg_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_words = []\n",
    "for i in range(len(polars)):\n",
    "    polar_words.append(polars[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Look over all reviews of restaurants.  You can determine which businesses are restaurants by looking in the `yelp_train_academic_dataset_business.json.gz` file from the ml project or downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    business_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "business_data = pd.DataFrame(business_data)\n",
    "valid_rows = business_data[business_data['categories'].apply(lambda x: 'Restaurants' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Each row of this file corresponds to a single business.  The category key gives a list of categories for each; take all where \"Restaurants\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "restaurant_ids = valid_rows['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the categories to check for spelling and capitalization\n",
    "grader.check(len(restaurant_ids) == 12876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The \"business_id\" here is the same as in the review data.  Use this to extract the review text for all reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         If you like lot lizards, you'll love the Pine ...\n",
       "1         Only went here once about a year and a half ag...\n",
       "2         Ate a Saturday morning breakfast at the Pine C...\n",
       "3         This is definitely not your usual truck stop. ...\n",
       "4         I like this location better than the one near ...\n",
       "                                ...                        \n",
       "144941    Barely open less than a week and I've been her...\n",
       "144942    Healthy Food that Keeps this Realtor on the Go...\n",
       "144943    So happy to have this healthy eatery option ri...\n",
       "144945    My new favorite restaurant.  They have 22 diff...\n",
       "144946    GreAt food awesome service . The best fish in ...\n",
       "Name: text, Length: 143361, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data2 = valid_rows.merge(data, how='left', on='business_id')\n",
    "data2 = data2.dropna(subset=['text'])\n",
    "restaurant_reviews = data2['text']\n",
    "restaurant_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just reviews of restaurants\n",
    "# restaurant_ids is helpful here\n",
    "grader.check(len(restaurant_reviews) == 143361)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I'll now find collocations --- that is, bigrams that are \"special\" and appear more often than we'd expect from chance. We can think of the corpus as defining an empirical distribution over all *n*-grams.  We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words. Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is the probability of the bigram $w_1 w_2$, then we want to look at word pairs $w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ \\frac{p(w_1 w_2)}{p(w_1) p(w_2)} $$\n",
    "\n",
    "### I'll also use smoothing parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,2), stop_words='english', min_df=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<143361x121409 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10425911 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_t = cv.fit_transform(restaurant_reviews); reviews_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = reviews_t.sum(axis=0)\n",
    "word_count = word_count.tolist()\n",
    "word_count = [val for sublist in word_count for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = dict(zip(word_list, word_count))\n",
    "grams = {k: v for k, v in sorted(grams.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11827661"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(grams.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams_prob = [i/11827661 for i in grams.values()]\n",
    "grams_with_probs = list(zip(grams, grams_prob)); grams_with_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams_smooth = [ (i + 1) / (11827661 + i*1) for i in grams.values()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at bigrams only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(stop_words='english',ngram_range=(2,2), min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_t2 = cv2.fit_transform(restaurant_reviews)\n",
    "word_list2 = cv2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count2 = reviews_t2.sum(axis=0)\n",
    "word_count2 = word_count2.tolist()\n",
    "word_count2 = [val for sublist in word_count2 for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = dict(zip(word_list2, word_count2))\n",
    "bigrams = {k: v for k, v in sorted(bigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "bigrams_prob = [i/11827661 for i in bigrams.values()]\n",
    "bigrams_with_probs = list(zip(bigrams, bigrams_prob)); bigrams_with_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at each word of the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = []\n",
    "\n",
    "for i in range(len(bigrams_with_probs)):\n",
    "    my_list.append(bigrams_with_probs[i][0].split())\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list2 = []\n",
    "\n",
    "for i in range(len(my_list)):\n",
    "    my_list2.append(my_list[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part = []\n",
    "for i in my_list2:\n",
    "    first_part.append(grams[i]/11827661)\n",
    "first_part    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## smoothing\n",
    "first_part = [(grams[i] + 10) / (11827661 + grams[i]*10) for i in my_list2]; first_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list4 = []\n",
    "\n",
    "for i in range(len(my_list)):\n",
    "    my_list4.append(my_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part = []\n",
    "for i in my_list4:\n",
    "    second_part.append(grams[i]/11827661)\n",
    "second_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## smoothing\n",
    "second_part = [(grams[i] + 10) / (11827661 + grams[i]*10) for i in my_list4]; second_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = list(zip(first_part, second_part))\n",
    "full2 = []\n",
    "for elem in full:\n",
    "    temp = elem[0]*elem[1]\n",
    "    full2.append(temp)\n",
    "full2\n",
    "## negative values here . needs smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = list(zip(bigrams_prob, full2)); final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = []\n",
    "for elem in final:\n",
    "    temp = elem[0] / elem[1]\n",
    "    ratios.append(temp)\n",
    "bigrams_ratios = list(zip(bigrams, ratios))\n",
    "the_list = sorted(bigrams_ratios, key=lambda x: x[1], reverse=True)\n",
    "top100 = the_list[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = []\n",
    "for elem in top100:\n",
    "    temp = elem[0]\n",
    "    top.append(temp)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "top100 = ['haricot vert'] * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
